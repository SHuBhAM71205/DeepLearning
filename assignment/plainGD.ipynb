{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b726860c",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2a9284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8054e",
   "metadata": {},
   "source": [
    "x_train the training data of type [28 x 28] x 60000\n",
    "\n",
    "y_train is the real output [0,1,5,7,....]x60000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "282e71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the mnist \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plt.imshow(x_train[0], cmap='gray')  # Use grayscale for MNIST\n",
    "# plt.title(f\"Label: {y_train[0]}\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# x_batch=x_train[0:10]\n",
    "# x_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b774ac5",
   "metadata": {},
   "source": [
    "Constansts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa20611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "L=3 #is the no if layer\n",
    "\n",
    "MAX_ITR=100000\n",
    "\n",
    "LEARNING_RATE=0.5 #Î· \n",
    "\n",
    "NO_OF_INPUT=[784,128,64,10] # mem friendly input h1,h2 output\n",
    "\n",
    "NO_OF_OUTPUT=10\n",
    "\n",
    "BATCH_SIZE=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1ebe61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 784)\n",
      "(64, 128)\n",
      "(10, 64)\n",
      "(1, 128)\n",
      "(1, 64)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_parameters(layer_dims, method=\"xavier\"):\n",
    "    \"\"\"\n",
    "    Initializes weights and biases for a fully connected neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layer_dims : list of int\n",
    "        Sizes of each layer in the network. Example: [784, 128, 64, 10]\n",
    "    method : str\n",
    "        Initialization method: \"xavier\" or \"he\"\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : list of np.ndarray\n",
    "        Weight matrices for each layer\n",
    "    biases : list of np.ndarray\n",
    "        Bias vectors for each layer\n",
    "    \"\"\"\n",
    "    weights = [] #(128,784) , (64,128) ,(10,64)\n",
    "    biases = [] #(764 x 1 , 128 x 1 , 64 x 1 , 10 x 1)\n",
    "    \n",
    "    for i in range(len(layer_dims)-1):\n",
    "        n_in = layer_dims[i]\n",
    "        n_out = layer_dims[i+1]\n",
    "        \n",
    "        if method == \"xavier\":\n",
    "            W = np.random.randn(n_out, n_in) * np.sqrt(1.0 / n_out)\n",
    "        elif method == \"he\":\n",
    "            W = np.random.randn(n_out,n_in) * np.sqrt(2.0 / n_out)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Use 'xavier' or 'he'.\")\n",
    "        \n",
    "        b = np.zeros((1, n_out))\n",
    "        \n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "weight,bias=initialize_parameters(NO_OF_INPUT)\n",
    "\n",
    "print(weight[0].shape)\n",
    "print(weight[1].shape)\n",
    "print(weight[2].shape)\n",
    "print(bias[0].shape)\n",
    "print(bias[1].shape)\n",
    "print(bias[2].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44ecb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=[]\n",
    "\n",
    "output=[]  \n",
    "\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def loss_fn(y_cal,y_org):\n",
    "    return -1*np.sum(y_org * np.log(y_cal + 1e-9))\n",
    "\n",
    "def eigen(output):\n",
    "    ans=np.eye(10)[output]\n",
    "    return ans\n",
    "\n",
    "def forwardpropogation(input): #(10,784)\n",
    "    activation,preactivation=[input],[]\n",
    "    #(10,784)  ()\n",
    "   \n",
    "    for i in range(L):\n",
    "        print(\"a,\",activation[i].shape,\"w,\",weight[i].shape,\"b,\",bias[i].shape)\n",
    "        preactivation.append(np.dot(weight[i],activation[-1].T) + bias[i].T)  # w0(128,784) , w1(64,128) ,w2(10,64) a0=(10,784) a1(10,128) a2(10,64)\n",
    "\n",
    "        #preac (128,10) (64,10) (10,10)\n",
    "        if i == L - 1:\n",
    "            A = softmax(preactivation[i])  # output layer \n",
    "        else:\n",
    "            A = sigmoid(preactivation[i])  # hidden layers (128,10)\n",
    "\n",
    "        activation.append(A.T)\n",
    "\n",
    "    return activation,preactivation # a0=(10,784) a1(10,128) a2(10,64) a3(10,10) #preac (128,10) (64,10) (10,10)\n",
    "\n",
    "\n",
    "def backpropogation(activation,preactivation,y):\n",
    "\n",
    "    dz=activation[-1]-y # foe the last layer  (10,10)\n",
    "    grad_w=[None] * L\n",
    "    grad_b=[None] * L\n",
    "    for i in reversed (range(L)):\n",
    "        A_prev=activation[i] # a3(10,10) a2(10,64)  a1(10,128) a0=(10,784)\n",
    "        grad_b.append(dz)\n",
    "        grad_w.append(np.dot(A_prev.T,dz))\n",
    "\n",
    "        if i > 0:  # for hidden layers\n",
    "            dA_prev = np.dot(weight[i].T, dZ)\n",
    "            dZ = dA_prev * sigmoid_derivative(preactivation[i-1])\n",
    "        \n",
    "    return grad_w,grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1265c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, (10, 784) w, (128, 784) b, (1, 128)\n",
      "a, (10, 128) w, (64, 128) b, (1, 64)\n",
      "a, (10, 64) w, (10, 64) b, (1, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,10) and (64,10) not aligned: 10 (dim 1) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m activation,preactivation=forwardpropogation(x_batch) \u001b[38;5;66;03m##a()  p()\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# loss=loss_fn(activation[-1],y_batch)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m dW,db=\u001b[43mbackpropogation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpreactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mbackpropogation\u001b[39m\u001b[34m(activation, preactivation, y)\u001b[39m\n\u001b[32m     45\u001b[39m A_prev=activation[i] \u001b[38;5;66;03m# a3(10,10) a2(10,64)  a1(10,128) a0=(10,784)\u001b[39;00m\n\u001b[32m     46\u001b[39m grad_b.append(dz)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m grad_w.append(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i > \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# for hidden layers\u001b[39;00m\n\u001b[32m     50\u001b[39m     dA_prev = np.dot(weight[i].T, dZ)\n",
      "\u001b[31mValueError\u001b[39m: shapes (10,10) and (64,10) not aligned: 10 (dim 1) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "#---NAIN--\n",
    "for epoch in range(MAX_ITR):\n",
    "    idx=np.arange(len(x_train))\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    x_train=x_train[idx] #(60000,28,28)\n",
    "    y_train=y_train[idx] #(60000)\n",
    "\n",
    "\n",
    "    for i in range(0,len(x_train),BATCH_SIZE):\n",
    "        x_batch=x_train[i : i+BATCH_SIZE]\n",
    "        x_batch = x_batch.reshape(x_batch.shape[0], -1) #(10,784)\n",
    "\n",
    "        y_batch=eigen(y_train[i:i+BATCH_SIZE]) #(10,10)\n",
    "\n",
    "        activation,preactivation=forwardpropogation(x_batch) ##a()  p()\n",
    "        \n",
    "        # loss=loss_fn(activation[-1],y_batch)\n",
    "\n",
    "        dW,db=backpropogation(activation,preactivation,y_batch)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
